{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "$id": "http://bioexcel.eu/biobb_ml/json_schemas/1.0/recurrent_neural_network",
    "name": "biobb_ml RecurrentNeuralNetwork",
    "title": "Wrapper of the TensorFlow Keras LSTM method.",
    "description": "Trains and tests a given dataset and save the complete model for a Recurrent Neural Network. Visit the LSTM documentation page in the TensorFlow Keras official website for further information.",
    "type": "object",
    "info": {
        "wrapped_software": {
            "name": "tensorflow",
            "version": ">2.1.0",
            "license": "MIT"
        },
        "ontology": {
            "name": "EDAM",
            "schema": "http://edamontology.org/EDAM.owl"
        }
    },
    "required": [
        "input_dataset_path",
        "output_model_path"
    ],
    "properties": {
        "input_dataset_path": {
            "type": "string",
            "description": "Path to the input dataset",
            "filetype": "input",
            "sample": "https://github.com/bioexcel/biobb_ml/raw/master/biobb_ml/test/data/neural_networks/dataset_recurrent.csv",
            "enum": [
                ".*\\.csv$"
            ],
            "file_formats": [
                {
                    "extension": ".*\\.csv$",
                    "description": "Path to the input dataset",
                    "edam": "format_3752"
                }
            ]
        },
        "output_model_path": {
            "type": "string",
            "description": "Path to the output model file",
            "filetype": "output",
            "sample": "https://github.com/bioexcel/biobb_ml/raw/master/biobb_ml/test/reference/neural_networks/ref_output_model_recurrent.h5",
            "enum": [
                ".*\\.h5$"
            ],
            "file_formats": [
                {
                    "extension": ".*\\.h5$",
                    "description": "Path to the output model file",
                    "edam": "format_3590"
                }
            ]
        },
        "output_test_table_path": {
            "type": "string",
            "description": "Path to the test table file",
            "filetype": "output",
            "sample": "https://github.com/bioexcel/biobb_ml/raw/master/biobb_ml/test/reference/neural_networks/ref_output_test_recurrent.csv",
            "enum": [
                ".*\\.csv$"
            ],
            "file_formats": [
                {
                    "extension": ".*\\.csv$",
                    "description": "Path to the test table file",
                    "edam": "format_3752"
                }
            ]
        },
        "output_plot_path": {
            "type": "string",
            "description": "Loss, accuracy and MSE plots",
            "filetype": "output",
            "sample": "https://github.com/bioexcel/biobb_ml/raw/master/biobb_ml/test/reference/neural_networks/ref_output_plot_recurrent.png",
            "enum": [
                ".*\\.png$"
            ],
            "file_formats": [
                {
                    "extension": ".*\\.png$",
                    "description": "Loss, accuracy and MSE plots",
                    "edam": "format_3603"
                }
            ]
        },
        "properties": {
            "type": "object",
            "properties": {
                "target": {
                    "type": "object",
                    "default": {},
                    "wf_prop": false,
                    "description": "Dependent variable you want to predict from your dataset. You can specify either a column name or a column index. Formats: { \"column\": \"column3\" } or { \"index\": 21 }. In case of mulitple formats, the first one will be picked."
                },
                "validation_size": {
                    "type": "number",
                    "default": 0.2,
                    "wf_prop": false,
                    "description": "Represents the proportion of the dataset to include in the validation split. It should be between 0.0 and 1.0.",
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05
                },
                "window_size": {
                    "type": "integer",
                    "default": 5,
                    "wf_prop": false,
                    "description": "Number of steps for each window of training model.",
                    "min": 0,
                    "max": 100,
                    "step": 1
                },
                "test_size": {
                    "type": "integer",
                    "default": 5,
                    "wf_prop": false,
                    "description": "Represents the number of samples of the dataset to include in the test split.",
                    "min": 0,
                    "max": 100000,
                    "step": 1
                },
                "hidden_layers": {
                    "type": "array",
                    "default": null,
                    "wf_prop": false,
                    "description": "List of dictionaries with hidden layers values. Format: [ { 'size': 50, 'activation': 'relu' } ]."
                },
                "optimizer": {
                    "type": "string",
                    "default": "Adam",
                    "wf_prop": false,
                    "description": "Name of optimizer instance. ",
                    "enum": [
                        "Adadelta",
                        "Adagrad",
                        "Adam",
                        "Adamax",
                        "Ftrl",
                        "Nadam",
                        "RMSprop",
                        "SGD"
                    ],
                    "property_formats": [
                        {
                            "name": "Adadelta",
                            "description": "Adadelta optimization is a stochastic gradient descent method that is based on adaptive learning rate per dimension to address two drawbacks: the continual decay of learning rates throughout training and the need for a manually selected global learning rate"
                        },
                        {
                            "name": "Adagrad",
                            "description": "Adagrad is an optimizer with parameter-specific learning rates; which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives; the smaller the updates"
                        },
                        {
                            "name": "Adam",
                            "description": "Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments"
                        },
                        {
                            "name": "Adamax",
                            "description": "It is a variant of Adam based on the infinity norm. Default parameters follow those provided in the paper. Adamax is sometimes superior to adam; specially in models with embeddings"
                        },
                        {
                            "name": "Ftrl",
                            "description": "Optimizer that implements the FTRL algorithm"
                        },
                        {
                            "name": "Nadam",
                            "description": "Much like Adam is essentially RMSprop with momentum; Nadam is Adam with Nesterov momentum"
                        },
                        {
                            "name": "RMSprop",
                            "description": "Optimizer that implements the RMSprop algorithm"
                        },
                        {
                            "name": "SGD",
                            "description": "Gradient descent -with momentum- optimizer"
                        }
                    ]
                },
                "learning_rate": {
                    "type": "number",
                    "default": 0.02,
                    "wf_prop": false,
                    "description": "Determines the step size at each iteration while moving toward a minimum of a loss function",
                    "min": 0.0,
                    "max": 100.0,
                    "step": 0.01
                },
                "batch_size": {
                    "type": "integer",
                    "default": 100,
                    "wf_prop": false,
                    "description": "Number of samples per gradient update.",
                    "min": 0,
                    "max": 1000,
                    "step": 1
                },
                "max_epochs": {
                    "type": "integer",
                    "default": 100,
                    "wf_prop": false,
                    "description": "Number of epochs to train the model. As the early stopping is enabled, this is a maximum.",
                    "min": 0,
                    "max": 1000,
                    "step": 1
                },
                "normalize_cm": {
                    "type": "boolean",
                    "default": false,
                    "wf_prop": false,
                    "description": "Whether or not to normalize the confusion matrix."
                },
                "remove_tmp": {
                    "type": "boolean",
                    "default": true,
                    "wf_prop": true,
                    "description": "Remove temporal files."
                },
                "restart": {
                    "type": "boolean",
                    "default": false,
                    "wf_prop": true,
                    "description": "Do not execute if output files exist."
                }
            }
        }
    },
    "additionalProperties": false
}